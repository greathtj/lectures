{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python ì–¸ì–´ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "- https://www.python.org/\n",
    "\n",
    "<img src=\"images/img01.png\" style=\"width:700px\"><br>\n",
    "\n",
    "<img src=\"images/image02.png\" style=\"width:700px\"><br>\n",
    "\n",
    "<img src=\"images/image03.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì´ì œ ë‹¤ìš´ë°›ì€ íŒŒì¼ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤. (ì£¼ì˜! ë°˜ë“œì‹œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "\n",
    "<img src=\"images/capture01.PNG\" style=\"width:500px\"><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì´ì œ pythonì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ì‹œë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/capture02.png\" style=\"width:700px\"><br>\n",
    "\n",
    "<img src=\"images/capture03.png\" style=\"width:700px\"><br>\n",
    "\n",
    "ì´ë ‡ê²Œ ë‚˜ì˜¤ë©´ ì´ì œ pythonì„ ì´ìš©í•  ì¤€ë¹„ê°€ ëë‚œ ê²ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Studio Codeë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://code.visualstudio.com/\n",
    "\n",
    "<img src=\"images/capture04.png\" style=\"width:700px\"><br>\n",
    "\n",
    "- ì„¤ì¹˜ê°€ ëë‚˜ë©´ python ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë„ë¡ í™˜ê²½ì„¤ì •ì„ í•´ì¤ë‹ˆë‹¤. (ê·¸ ë°©ë²•ì€?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yolov8ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í™˜ê²½ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìš°ì„  git í”„ë¡œê·¸ë¨ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.<br>\n",
    "\n",
    "https://git-scm.com/\n",
    "\n",
    "<img src=\"images/capture05.png\" style=\"width:700px\"><br>\n",
    "\n",
    "* ë¨¼ì € ê°•ì˜ìë£Œ ì €ì¥ì†Œì— ì ‘ì†í•©ë‹ˆë‹¤.<br>\n",
    "\n",
    "https://github.com/greathtj/lectures\n",
    "\n",
    "<img src=\"images/capture06.png\" style=\"width:700px\"><br>\n",
    "\n",
    "* powershellì„ ì´ìš©í•´ì„œ í•´ë‹¹ ì €ì¥ì†Œë¥¼ clone í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"images/capture07.png\" style=\"width:600px\"><br>\n",
    "\n",
    "\n",
    "* ì´ì œëŠ” YoloV8 ê³µì‹ ì‚¬ì´íŠ¸ì— ì ‘ì†í•©ë‹ˆë‹¤. (https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "<img src=\"images/capture08.png\" style=\"width:700px\"><br>\n",
    "\n",
    "* ê°•ì˜ìë£Œ í´ë” ì•ˆì— cloneì„ í•©ë‹ˆë‹¤. (\"c:\\lectures\")\n",
    "\n",
    "git clone https://github.com/ultralytics/yolov5.git ì‹¤í–‰<br>\n",
    "í•˜ì§€ë§Œ, ìš°ë¦¬ì˜ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê¼­ cloneì´ í•„ìš”í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ì œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(ultralytics)ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë¨¼ì € vscodeë¥¼ ì´ìš©í•´ì„œ ì‘ì—…í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"images/capture09.png\" style=\"width:700px\"><br>\n",
    "\n",
    "* ìƒˆ í„°ë¯¸ë„ì„ ì—´ì–´ì„œ ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "> python -m venv yv8env<br>\n",
    "> yv8env/Scripts/Activate<br>\n",
    "> pip install ultralytics<br>\n",
    "\n",
    "<img src=\"images/capture10.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ì œ OpenCVë¥¼ ê±´ë“œë ¤ ë³¼ê¹Œìš”?<br>\n",
    "\n",
    "- ê·¸ë¦¼ì„ ë¶ˆëŸ¬ì„œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/greathtj/Sync/MyProject/4_kitech/yolov8/yv8env/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "frame = cv2.imread(\"2nd_test.jpg\")\n",
    "\n",
    "cv2.imshow(\"image view\", frame)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì›¹ìº ì´ ìˆë‹¤ë©´ ì‹¤ì‹œê°„ ë™ì˜ìƒì„ ë³¼ ìˆ˜ë„ ìˆì£ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/greathtj/Sync/MyProject/4_kitech/yolov8/yv8env/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"webcam test\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key > 0:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¸ë¦¬ê³  80ê°œì˜ ì‚¬ë¬¼ì— ëŒ€í•´ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ì‹¤í–‰ì‹œì¼œ ë´…ë‹ˆë‹¤.\n",
    "* coco datasetì„ ì´ìš©í•´ì„œ ë¯¸ë¦¬ í•™ìŠµí•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n",
    "\n",
    "        The COCO (Common Objects in Context) dataset is a widely used benchmark dataset for object detection, instance segmentation, and keypoint detection tasks in computer vision. It provides a large-scale dataset with rich annotations, covering a wide variety of object categories in everyday scenes.\n",
    "\n",
    "        The COCO dataset consists of images from complex everyday scenes, such as street scenes, indoor environments, and natural images. The objects in the images are annotated with bounding boxes, segmentations, and keypoints, providing detailed information about their locations, shapes, and poses. The dataset includes over 200,000 labeled objects across 80 different object categories, making it suitable for training and evaluating object detection and segmentation models.\n",
    "\n",
    "        COCO dataset annotations also include additional information, such as object attributes, relationships between objects, and captions describing the scenes. This makes the dataset valuable not only for object detection tasks but also for tasks related to image understanding and natural language processing.\n",
    "\n",
    "        The COCO dataset is widely used in research and development of computer vision algorithms and serves as a benchmark for evaluating the performance of object detection and segmentation models. It has spurred advancements in various computer vision tasks and has become a standard dataset for evaluating state-of-the-art models in the field.\n",
    "\n",
    "<img src=\"images/capture11.png\" style=\"width:700px\">\n",
    "\n",
    "* coco dataset ëª¨ë¸ì„ ì´ìš©í•œ ì¸ì‹ì€ yolov8 ì„¤ì¹˜ (pip install ultralytics) í›„ ë§Œë“¤ì–´ì§„ yolo ë¼ëŠ” í”„ë¡œê·¸ë¨ì„ ì´ìš©í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ìˆ˜í–‰ ëª…ë ¹ì–´ ì…ë ¥ì€ í„°ë¯¸ë„ ì°½ì— ì•„ë˜ì™€ ê°™ì´ ì…ë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "        yolo predict model='yolov8n.pt' source='1st_test.jpg'\n",
    "\n",
    "        \n",
    "* í•˜ì§€ë§Œ ì´ ëª…ë ¹ì„ ìˆ˜í–‰í•˜ë ¤ë©´ \"vs_redist.x64.exe\"ë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"images/capture12.png\" style=\"width:700px\">\n",
    "\n",
    "* What is Microsoft Visual C++ Redistributable?\n",
    "\n",
    "        Microsoft Visual C++ Redistributable is a collection of runtime components and libraries that are required by certain software applications developed using Microsoft Visual C++. These redistributable packages provide essential runtime support for applications to run properly on Windows operating systems.\n",
    "\n",
    "        When a software program is built using Microsoft Visual C++, it relies on certain dynamic link libraries (DLLs) and runtime components. The Microsoft Visual C++ Redistributable packages include these DLLs and runtime components, ensuring that the necessary dependencies are available on the target system where the application will be installed and run.\n",
    "\n",
    "        The Visual C++ Redistributable packages are installed on the user's system either by the software program itself during installation or by the operating system when required by an application. The packages may include multiple versions of the Visual C++ runtime, as different software applications may depend on different versions.\n",
    "\n",
    "        Having the appropriate version of the Microsoft Visual C++ Redistributable installed on a system ensures that software programs built with Visual C++ can run smoothly, without encountering DLL-related errors or missing runtime dependencies.\n",
    "\n",
    "        It's worth noting that different software programs may require different versions of the Visual C++ Redistributable, so it's common to have multiple versions installed on a system to support various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/greathtj/Sync/MyProject/4_kitech/yolov8/yv8env/lib/python3.11/site-packages/cv2/qt/plugins\"\n",
      "Ultralytics YOLOv8.0.123 ğŸš€ Python-3.11.2 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 7974MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "image 1/1 /home/greathtj/Sync/MyProject/4_kitech/yolov8/1st_test.jpg: 480x640 3 persons, 4 bicycles, 12 cars, 86.4ms\n",
      "Speed: 4.3ms preprocess, 86.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model='yolov8n.pt' source='1st_test.jpg' show=true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* runs/detect/predict# í´ë”ì— ê°€ë©´ ë°©ê¸ˆ ì‹¤í–‰í•œ ë‚´ìš©ì´ ê²°ê³¼ ì‚¬ì§„ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (1st_test.jpg)\n",
    "\n",
    "<img src=\"images/capture13.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ì˜ìƒì„ ì´ìš©í•´ ë´…ì‹œë‹¤. (ìš°ì„  yolo í”„ë¡œê·¸ë¨ì„ ì´ìš©í•´ì„œ í•´ë³´ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model='yolov8n.pt' source=0 show=true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predictí•œ ê²°ê³¼ë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ ì½”ë“œ (ìŒ... ì¢€ ë³µì¡í•˜ê¸´í•œë°...ì©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from ultralytics.yolo.utils.plotting import Annotator\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "# model = YOLO(\"custom_trained/gates_jobs_musk/best.pt\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = model.predict(source=img, save=False, show=False, conf=0.3)\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            annotator = Annotator(frame)\n",
    "            for box in boxes:\n",
    "                b = box.xyxy[0]  # get box coordinates in (top, left, bottom, right) format\n",
    "                c = box.cls\n",
    "                cnf = box.conf.item()\n",
    "                b_caption = model.names[int(c)] + f\"_({cnf:.2f})\"\n",
    "                annotator.box_label(b, b_caption)\n",
    "\n",
    "        frame = annotator.result() \n",
    "        cv2.imshow(\"yolov8\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key > 0:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì, ì´ì œëŠ” ë‚´ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¼ ì°¨ë¡€.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ì‚¬ì§„ì„ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- ì¸ì‹í•˜ê³ ì í•˜ëŠ” ì‚¬ë¬¼ì˜ íŠ¹ì„±ì„ ì˜ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- ë°°ê²½ì„ ë‹¤ì–‘í•˜ê²Œ í•˜ëŠ”ê²Œ ì¤‘ìš”í•©ë‹ˆë‹¤. (ë°°ê²½ì´ ë‹¤ì–‘í•´ì•¼ ì¸ì‹í•˜ê³ ì í•˜ëŠ” ì œí’ˆë§Œì˜ íŠ¹ì§•ì„ ì˜ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
    "- ì¶©ë¶„í•œ ì–‘ì˜ ì‚¬ì§„ì„ ì¤€ë¹„í•˜ëŠ” ê²Œ ì¤‘ìš”í•©ë‹ˆë‹¤. (ì‚¬ë¬¼ë‹¹ ìµœì†Œ 300ì¥ ì´ìƒ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ë¼ë²¨ë§ì„ í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "    * Roboflow ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ë´…ì‹œë‹¤. (https://roboflow.com/)\n",
    "    * ì•ˆë‚´ì— ë”°ë¼ ê°€ì…í•˜ê³  Signin í•˜ì„¸ìš”. (Google idë¡œ ê°€ì… ê°€ëŠ¥)\n",
    "    \n",
    "    * í”„ë¡œì íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.<br>\n",
    "    <img src=\"images/capture14.png\" style=\"width:700px\"><br>\n",
    "    <img src=\"images/capture15.png\" style=\"width:700px\">\n",
    "\n",
    "    * íŒŒì¼ë“¤ì„ ëŒì–´ë‹¤ ë†”ì„œ upload í•©ë‹ˆë‹¤.<br>\n",
    "    <img src=\"images/capture16.png\" style=\"width:700px\">\n",
    "\n",
    "    * ì´ì œ ê°ê°ì˜ íŒŒì¼ì„ ì—´ì–´ì„œ annotation (í˜¹ì€ ë¼ë²¨ë§)ì„ ì§„í–‰í•©ë‹ˆë‹¤. (ì—„ì²­ë‚œ ë…¸ê°€ë‹¤... ê·¸ëŸ¬ë‚˜... ì°¸ì„ì„± ìˆê²Œ..)<br>\n",
    "    <img src=\"images/capture17.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ë¼ë²¨ë§ì´ ë‹¤ ë˜ë©´ ì´ì œ ë°ì´í„°ì…‹ì„ export í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"images/capture18.png\" style=\"width:700px\">\n",
    "\n",
    "* ë¬¼ë¡  ì´ë•Œ ìš°ë¦¬ëŠ” yolov8ì„ ì„ íƒí•©ë‹ˆë‹¤.<br>\n",
    "<img src=\"images/capture19.png\" style=\"width:700px\">\n",
    "\n",
    "* ë°ì´í„° ì…‹ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ê±°ë‚˜, ë‹¤ìš´ë¡œë“œ ì½”ë“œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"W6heiWaOfMtbe0JEgZzt\")\n",
    "project = rf.workspace(\"korea-institute-of-industrial-technology\").project(\"face-jogph\")\n",
    "dataset = project.version(6).download(\"yolov8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ì´ì œ data.yaml íŒŒì¼ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"images/capture20.png\" style=\"width:700px\"><br>\n",
    "\n",
    "<img src=\"images/capture21.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë‹¤ìš´ë°›ì€ ê²½ìš°ëŠ” ë°ì´í„°ì…‹ ì••ì¶•íŒŒì¼ì„ í’€ì–´ì„œ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì™„ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"images/capture22.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í•´ë‹¹ í´ë”ë¥¼ ë°˜ì˜í•˜ì—¬ data.yaml íŒŒì¼ì„ í¸ì§‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. (ë‹¨, GPUê°€ ì„¤ì¹˜ëœ ì»´í“¨í„°ê°€ í•„ìš”í•  ê²ë‹ˆë‹¤. ê·¸ë¦¬ê³  GPU êµ¬ë™ í™˜ê²½ë„ ë§Œë“¤ì–´ì¤˜ì•¼ê² ì£ ... ã… ã… )\n",
    "\n",
    "- í•™ìŠµ ì½”ë“œ<br>\n",
    "\n",
    "* Build a new model from YAML and start training from scratch<br>\n",
    "    >yolo detect train data='face-6/data.yaml' model=yolov8n.yaml epochs=100 imgsz=640\n",
    "\n",
    "* Start training from a pretrained *.pt model<br>\n",
    "    >yolo detect train data='face-6/data.yaml' model=yolov8n.pt epochs=100 imgsz=640\n",
    "\n",
    "* Build a new model from YAML, transfer pretrained weights to it and start training<br>\n",
    "    >yolo detect train data='face-6/data.yaml' model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/capture23.png\" style=\"width:700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectionì„ ìœ„í•œ ê¸°ë³¸ ì½”ë“œë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤.\n",
    "* í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ëë‚˜ë©´ 'best.pt'ë¼ëŠ” íŒŒì¼ì´ ë§Œë“¤ì–´ì¡Œì„ ê²ë‹ˆë‹¤.\n",
    "* ì´ê±¸ ì´ìš©í•´ì„œ ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ detection (or prediction)ì„ í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from ultralytics.yolo.utils.plotting import Annotator\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = model.predict(source=img, save=False, show=False, conf=0.3)\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            annotator = Annotator(frame)\n",
    "            for box in boxes:\n",
    "                b = box.xyxy[0]  # get box coordinates in (top, left, bottom, right) format\n",
    "                c = box.cls\n",
    "                cnf = box.conf.item()\n",
    "                b_caption = model.names[int(c)] + f\"_({cnf:.2f})\"\n",
    "                annotator.box_label(b, b_caption)\n",
    "\n",
    "        frame = annotator.result() \n",
    "        cv2.imshow(\"yolov8\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key > 0:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
