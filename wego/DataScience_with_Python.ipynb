{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb40ccd5",
   "metadata": {},
   "source": [
    "## 제조 AI를 위한 데이터 전처리 그리고 모델 설계\n",
    "\n",
    "데이터가 잘 모이고 있다고 가정하고, 이제 데이터를 정리하고 또 인공지능으로 변모시켜 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bcdf0",
   "metadata": {},
   "source": [
    "## 2. 데이터의 전처리 그리고 피처 엔지니어링 (혹은 마사지(?))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378eb1b8",
   "metadata": {},
   "source": [
    "### a. Python 설치\n",
    "- 데이터 사이언스를 수행해 가는 과정에서 Python은 데이터 분석, 머신러닝, 시각화 등 다양한 작업을 위한 핵심 도구이다.\n",
    "- NumPy, Pandas, Scikit-learn과 같은 강력한 라이브러리 덕분에 복잡한 데이터 처리와 모델링을 효율적으로 수행할 수 있다. \n",
    "- 접근성이 높고 다양한 분야의 커뮤니티가 활성화되어 있어 협업과 지식 공유가 용이하다는 점도 큰 장점이다.\n",
    "<br><br>\n",
    "- 그리고 결정적으로... <i>배우기가 그 어떤 coding language 보다 쉽다.</i>\n",
    "\n",
    "<img src=\"images/image_surprise.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c831e",
   "metadata": {},
   "source": [
    "- 다음 사이트에 가서 적당한 버전을 설치하면 바로 사용할 수 있다.\n",
    "    - https://www.python.org/\n",
    "    - 그리고 이점 꼭 주의하시길!!!\n",
    "\n",
    "        <img src=\"images/image_017.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8ccc3",
   "metadata": {},
   "source": [
    "- Python이 설치가 되었다면 이제는 Visual Studio Code를 설치하자.\n",
    "    - 코딩을 처음 시작하는 분들에게 VS Code는 가볍고 강력한 무료 코드 편집기이다. 다양한 프로그래밍 언어를 지원하며, 특히 Python 작업을 하는 데 없어서는 안 될 필수 프로그램이다.\n",
    "    - 다양한 확장 프로그램: VS Code의 가장 큰 장점 중 하나는 방대한 확장 프로그램 생태계이다. Python 확장 프로그램을 설치하면 Jupyter Notebook, 데이터 시각화, Git 연동 등 다양한 기능을 추가해 나만의 맞춤형 개발 환경을 구축할 수 있다.\n",
    "    - 아무튼 거의 <b>만병통치약</b>이라는 느낌이다. \"묻지 말고 따지지도 말고\" 그냥 설치하자... (https://code.visualstudio.com/)\n",
    "\n",
    "- 가상환경 구축하기\n",
    "    - Python으로 개발할 때 가상 환경을 만드는 것은 프로젝트의 안정성과 독립성을 보장하기 위한 필수적인 작업이다.\n",
    "    - 가상 환경은 각 프로젝트마다 고유한 Python 환경을 만들어, 서로 다른 프로젝트 간에 설치된 라이브러리들이 충돌하는 것을 방지한다.\n",
    "    - 프로젝트 종속성 관리: 가상 환경을 사용하면 해당 프로젝트에 필요한 라이브러리만 깔끔하게 관리할 수 있다. pip freeze > requirements.txt 명령어를 사용해 프로젝트에 사용된 모든 라이브러리 목록을 requirements.txt 파일로 쉽게 만들 수 있다. 이 파일만 있으면 다른 개발자나 새로운 환경에서도 동일한 개발 환경을 손쉽게 구축할 수 있어 협업과 배포가 용이해진다.\n",
    "    - 전역 환경 오염 방지: 가상 환경 없이 라이브러리를 설치하면, 시스템 전역에 라이브러리가 설치되어 다른 프로젝트나 심지어 시스템 자체의 Python 환경에 영향을 줄 수 있는데 이는 예기치 않은 오류를 유발할 수 있다. 가상 환경은 이 문제를 해결하여 시스템의 기본 Python 환경을 깨끗하게 유지해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb2e2d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!python -m venv .venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f69a2",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!.venv/Sripts/Activate.ps1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cd137",
   "metadata": {},
   "source": [
    "- 이제 설치가 끝났으니 여러분의 쥬피터 노트북 (jupyter notebook) 파일의 각 셀들을 실행해 볼 수 있습니다. 자 시작해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea37436",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2cf1d",
   "metadata": {},
   "source": [
    "### b. AI 모델 수립을 위한 데이터 사이언스 로드맵\n",
    "- 문제 정의 및 목표 설정 (Problem Definition & Goal Setting)\n",
    "    - 목표 명확화: 해결하고자 하는 품질 또는 제조공정상의 문제를 구체적으로 정의한다. 어떤 예측(예: 고장예측), 분류(예: 불량 및 이상 분류), 또는 분석(예: 공정조건 변화에 따른 품질의 변화 상관관계)을 할 것인지 명확히 한다.\n",
    "    - 지표 설정: 모델의 성능을 어떻게 평가할 것인지 기준을 세운다. 정확도(Accuracy), 정밀도(Precision), 재현율(Recall) 등 구체적인 지표를 설정하여 모델 개발 방향을 잡는다.\n",
    "- 데이터 수집 (Data Collection)\n",
    "    - 센서는 뭘로?\n",
    "    - 기존에 축적된 디지털 데이터가 있는가?\n",
    "    - 디지털화할 수 있는 아날로그(수작업) 데이터가 있는가?\n",
    "- 데이터 전처리 (Data Preprocessing)\n",
    "    - 수집된 데이터를 모델이 이해할 수 있도록 다듬는 작업이다. 이 단계가 모델의 성능을 크게 좌우한다.\n",
    "    - 결측치 처리: 데이터의 빈 부분을 채우거나 제거한다.\n",
    "    - 이상치 처리: 데이터의 정상 범위를 벗어나는 특이한 값(outliers)을 찾아 분석하거나 제거한다.\n",
    "    - 데이터 정규화/표준화: 다양한 스케일을 가진 데이터들을 일정한 범위로 맞춰 모델 학습에 적합한 형태로 만든다. 이걸 안하면 엉뚱한 방향으로...\n",
    "- 탐색적 데이터 분석 (EDA: Exploratory Data Analysis)\n",
    "    - 데이터를 깊이 이해하는 과정이다. 그냥 일단 본다. 뭐 어떻게 생겼는지... 이때 구체적 분석의 방향이나 아이디어도 떠오를 수 있다. 순전히 운이다!!\n",
    "    - 데이터 시각화: 차트나 그래프를 그려 데이터의 분포, 변수 간의 관계, 숨겨진 패턴을 시각적으로 파악한다. (노가다가 너희를 자유롭게 하리라.)\n",
    "    - 통계적 분석: 데이터의 핵심적인 특징을 이해하기 위해 평균, 분산, 상관관계 등을 계산한다. (대체로는 아무것도 안나올 가능성이 많다!!! 그래서 피처 엔지니어링이 필요하다.)\n",
    "- 피처 엔지니어링 (Feature Engineering)\n",
    "    - 모델의 성능을 향상시키기 위해 새로운 변수를 만드는 창의적인 단계이다. (사실은 더 심한 노가다일수도...쩝)\n",
    "    - 새로운 변수 생성: 기존 변수들을 조합하거나 변환하여 모델이 더 잘 학습할 수 있는 의미 있는 특성(features)을 만든다. 예를 들어, 진동이나 소음데이터에서 데시벨(진동압 혹은 음압) 값의 변화로 변환시켜본다든지, 진폭 데이터를 FFT (Fast Fourier Transformation) 해서 주파수 데이터로 변환(time domain - frequency domain)시켜본다든지. (하여간 아는 거는 다 동원해 본다. 그래도 아무것도 안나오면 그냥 팔자겠거니....)\n",
    "- 모델링 (Modeling)\n",
    "    - 이제 데이터를 기반으로 AI 모델을 만드는 핵심 단계이다. (하지만 이것도 내가 딱히 아이디어를 넣기는 쉽지 않다. 어떤 알고리즘을 써야 할지는 뭐 거의 정해져 있다.)\n",
    "    - 모델 선택: 문제 유형에 맞는 머신러닝/딥러닝 알고리즘(예: 선형 회귀, 랜덤 포레스트, 합성곱 신경망)을 선택한다.\n",
    "    - 모델 학습: 준비된 데이터를 이용해 모델을 훈련시킨다. (좋은 GPU가 있으면 금상첨와, 없어도 상관없다.. 시간이 많이 걸릴 뿐이다. 우리가 머리가 없지 시간이 없나.)\n",
    "    - 모델 검증 및 튜닝: 학습된 모델의 성능을 평가하고, 하이퍼파라미터(모델 학습에 영향을 미치는 설정값)를 조정하여 최적의 성능을 끌어낸다. (대부분은 이걸로 논문 쓰면 된다.)\n",
    "- 모델 배포 및 운영 (Deployment & Operation)\n",
    "    - 만들어진 모델이 실제로 작동하게 하는 것이다.\n",
    "    - 모델 배포: 완성된 모델을 실제 서비스 환경(예: 웹어플리케이션, 혹은 데스크탑 애플리케이션)에 적용한다.\n",
    "    - 지속적 모니터링: 배포된 모델의 성능이 시간이 지나면서 저하되지 않는지 계속해서 확인하고, 필요 시 다시 학습시킨다. (노가다는 끝나지 않는다.)\n",
    "\n",
    "이 과정을 통해 데이터에서 가치를 창출하고, 제조현장의 공정 및 품질 문제를 해결하거나 불량이 발생할 것을 예측하는 AI 모델이 탄생하게 된다. 각 단계는 서로 유기적으로 연결되어 있으며, 문제의 복잡성에 따라 반복적인 과정을 거치게 된다. (즉, 노가다다...ㅠㅠ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f76de",
   "metadata": {},
   "source": [
    "### c. 기초실습 I\n",
    "- 아래의 셀들을 하나씩 수행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd01636",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49d7cc",
   "metadata": {},
   "source": [
    "- 단순히 파일을 읽어서 그 내용을 보여주는 코드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CSV 파일 경로 설정\n",
    "# 현재 스크립트 파일과 동일한 폴더에 해당 파일이 있다고 가정합니다.\n",
    "file_path = 'data/view/data_ex_02.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 데이터 프레임의 내용을 화면에 출력\n",
    "    print(f\"'{file_path}' 파일의 내용:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{file_path}' 파일을 찾을 수 없습니다.\")\n",
    "    print(\"CSV 파일이 스크립트와 동일한 위치에 있는지 확인해 주세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"파일을 읽는 도중 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1945d9",
   "metadata": {},
   "source": [
    "- 이번에는 그래프를 그려볼까? (걷지도 못하는데 뛰라고???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5638d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그래프 생성\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(df['_time'], df['x'])\n",
    "plt.title('vib. X vs. Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Vib. x')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 그래프를 화면에 보여주기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8f963",
   "metadata": {},
   "source": [
    "- 전체 데이터를 다 보여주고 싶은데, 너무 많아서 그래프를 그릴 수가 없다. 어떡하지???\n",
    "    - 그래, 1분동안의 데이터를 평균내서 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 불러오기\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 원본 데이터 파일 불러오기\n",
    "\n",
    "# 예시 데이터 파일 경로\n",
    "file_path = 'data/Accelermeter_SEN002_2023-05-16T07.csv'\n",
    "\n",
    "# 파일 열기\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1분 단위로 평균 내기\n",
    "\n",
    "# 일단 시간 형식으로 데이터를 변환합니다.\n",
    "df['_time'] = pd.to_datetime(df['_time'], errors='coerce')\n",
    "df.set_index('_time', inplace=True)\n",
    "\n",
    "# 데이터를 1분 단위로 재표본(resample)하여 평균을 계산합니다.\n",
    "# 'T'는 'minute'을 의미합니다.\n",
    "minutely_avg = df.resample('1T').mean()\n",
    "\n",
    "# 1분 단위 평균 데이터를 출력합니다.\n",
    "print(\"1분 단위 평균 데이터:\")\n",
    "print(minutely_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분 단위 평균 데이터를 그래프로 시각화합니다.\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(minutely_avg.index, minutely_avg['x'], marker='o', linestyle='-', color='skyblue')\n",
    "\n",
    "# 그래프 제목과 라벨 설정\n",
    "plt.title('Average by one minute', fontsize=16)\n",
    "plt.xlabel('time', fontsize=12)\n",
    "plt.ylabel('value', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout() # 라벨이 잘리지 않도록 레이아웃 조정\n",
    "\n",
    "# 그래프를 화면에 보여주기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8043f",
   "metadata": {},
   "source": [
    "- 자, 이제 좀더 알아보기 쉽게 만들어 볼까요?\n",
    "    - 아니, 사실은 \"알아보기 쉽게\"가 아니라, 뭔가 좀더 프로페셔날하게 화장을 하는 건데...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 함수를 먼저 만들고...\n",
    "\n",
    "def read_data(file_path):\n",
    "    # 파일 열기\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def make_average(df:pd.DataFrame):\n",
    "    work_df = df.copy()\n",
    "    # 일단 시간 형식으로 데이터를 변환합니다.\n",
    "    work_df['_time'] = pd.to_datetime(work_df['_time'], errors='coerce')\n",
    "    work_df.set_index('_time', inplace=True)\n",
    "\n",
    "    # 데이터를 1분 단위로 재표본(resample)하여 평균을 계산합니다.\n",
    "    # 'T'는 'minute'을 의미합니다.\n",
    "    minutely_avg = work_df.resample('1T').mean()\n",
    "\n",
    "    # 1분 단위 평균 데이터를 출력합니다.\n",
    "    print(\"1분 단위 평균 데이터:\")\n",
    "    print(minutely_avg)\n",
    "\n",
    "    return minutely_avg\n",
    "\n",
    "def plot_graph(df:pd.DataFrame, col_key):\n",
    "    # 1분 단위 평균 데이터를 그래프로 시각화합니다.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(df.index, df[col_key], marker='o', linestyle='-', color='skyblue')\n",
    "    # plt.scatter(df.index, df[col_key], linestyle='-', color='skyblue')\n",
    "\n",
    "    # 그래프 제목과 라벨 설정\n",
    "    plt.title('Stats by one minute', fontsize=16)\n",
    "    plt.xlabel('time', fontsize=12)\n",
    "    plt.ylabel('value', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout() # 라벨이 잘리지 않도록 레이아웃 조정\n",
    "\n",
    "    # 그래프를 화면에 보여주기\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이게 Main 프로그램이죠.\n",
    "\n",
    "df = read_data(\"data/Accelermeter_SEN002_2023-05-16T07.csv\")\n",
    "ave_df = make_average(df)\n",
    "plot_graph(ave_df, \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db94ab7",
   "metadata": {},
   "source": [
    "- 평균을 그려봤으니, 이제 뭘 해봐야하죠?\n",
    "    - 그렇죠 표준편차를 그려봐야겠네요...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_statistics(df:pd.DataFrame):\n",
    "    work_df = df.copy()\n",
    "\n",
    "    # 일단 시간 형식으로 데이터를 변환합니다.\n",
    "    work_df['_time'] = pd.to_datetime(work_df['_time'], errors='coerce')\n",
    "    work_df.set_index('_time', inplace=True)\n",
    "\n",
    "    # 데이터를 1분 단위로 재표본(resample)하여 평균과 표준편차를 계산합니다.\n",
    "    minutely_stats = work_df.resample('1min').agg(['mean', 'std'])\n",
    "    # print(minutely_stats)\n",
    "    \n",
    "    # 열 이름 재구성\n",
    "    minutely_stats.columns = ['_'.join(col).strip() for col in minutely_stats.columns.values]\n",
    "    \n",
    "\n",
    "    # 1분 단위 평균, 표준편차 데이터를 출력합니다.\n",
    "    print(\"Minutely std data:\")\n",
    "    print(minutely_stats)\n",
    "    print(minutely_stats.columns.values)\n",
    "\n",
    "    return minutely_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_data(\"data/Accelermeter_SEN002_2023-05-16T07.csv\")\n",
    "state_df = make_statistics(df)\n",
    "print(state_df.head())\n",
    "\n",
    "plot_graph(state_df, \"x_std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b45b8",
   "metadata": {},
   "source": [
    "- 두개의 그래프를 한꺼번에 볼 수는 없나요????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfed76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_all(df:pd.DataFrame):\n",
    "    # 두 개의 서브플롯을 생성하여 평균과 표준편차를 각각 그립니다.\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 12), sharex=True)\n",
    "    \n",
    "    # 첫 번째 서브플롯: 평균 그래프\n",
    "    ave_axes:plt.Axes = axes[0]\n",
    "    ave_axes.plot(df.index, df['x_mean'], marker='o', linestyle='-', color='skyblue')\n",
    "    ave_axes.set_title('Minutely ave data', fontsize=16)\n",
    "    ave_axes.set_ylabel('Ave', fontsize=12)\n",
    "    ave_axes.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # 두 번째 서브플롯: 표준편차 그래프\n",
    "    std_axes:plt.Axes = axes[1]\n",
    "    std_axes.plot(df.index, df['x_std'], marker='o', linestyle='-', color='salmon')\n",
    "    std_axes.set_title('Minutely std data', fontsize=16)\n",
    "    std_axes.set_xlabel('Time', fontsize=12)\n",
    "    std_axes.set_ylabel('STD', fontsize=12)\n",
    "    std_axes.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout() # 라벨이 잘리지 않도록 레이아웃 조정\n",
    "    \n",
    "    # 그래프를 화면에 보여주기\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96764a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_all(state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c705fb1",
   "metadata": {},
   "source": [
    "- 작업한 내용을 파일로 저장하면, 그러면 마무리 되는거죠..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과를 CSV 파일로 저장하는 함수\n",
    "def save_to_csv(df:pd.DataFrame, filename):\n",
    "    \"\"\"\n",
    "    데이터프레임을 CSV 파일로 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 저장할 데이터프레임.\n",
    "        filename (str): 저장할 파일 이름.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(filename)\n",
    "        print(f\"\\n데이터가 '{filename}' 파일에 성공적으로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n오류: 파일을 저장하는 도중 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minutely_stats를 'minutely_stats.csv' 파일로 저장합니다.\n",
    "save_to_csv(state_df, 'minutely_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e01855",
   "metadata": {},
   "source": [
    "### d. 기초실습 II - 결측 데이터 처리\n",
    "\n",
    "- 평균/분산만 있는 것은 아니다. 데이터 빠짐, 즉 결측치를 다루는 작업을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = read_data(\"data/missing_values.csv\")\n",
    "\n",
    "# 일단 시간 형식으로 데이터를 변환합니다.\n",
    "missing_df['_time'] = pd.to_datetime(missing_df['_time'], errors='coerce')\n",
    "missing_df.set_index('_time', inplace=True)\n",
    "\n",
    "plot_graph(missing_df, \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae581f",
   "metadata": {},
   "source": [
    "- 결측치를 채워보자.\n",
    "    - 일단은 단순하게 접근하는게 제일 좋지 않을까... 그냥 평균이나, 0, 혹은 중간값???\n",
    "    - 이 방법은 데이터의 분포가 정규 분포에 가깝거나, 결측치가 무작위로 발생했을 때 효과적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70740b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df = missing_df.copy()\n",
    "\n",
    "# 'x' 컬럼의 결측치를 평균값으로 채우기\n",
    "# filled_df['x'] = filled_df['x'].fillna(filled_df['x'].mean())\n",
    "# print(\"결측치를 평균값으로 채운 후:\")\n",
    "\n",
    "# 'x' 컬럼의 결측치를 0으로 채우기\n",
    "# filled_df['x'] = filled_df['x'].fillna(0)\n",
    "# print(\"결측치를 평균값으로 채운 후:\")\n",
    "\n",
    "# 'x' 컬럼의 결측치를 중간값(median)으로 채우기\n",
    "filled_df['x'] = missing_df['x'].fillna(missing_df['x'].median())\n",
    "print(\"결측치를 중간값으로 채운 후:\")\n",
    "\n",
    "plot_graph(filled_df, \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a6ee1",
   "metadata": {},
   "source": [
    "- 이전값 혹은 이후값을 채우는 방법도 있다.\n",
    "    - 시계열 데이터인 경우 이런 방법이 많이 쓰인다. 하지만...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df = missing_df.copy()\n",
    "\n",
    "# 이전 값으로 채우기 (forward fill)\n",
    "filled_df['x'] = filled_df['x'].ffill()\n",
    "print(\"결측치를 이전 값으로 채운 후:\")\n",
    "\n",
    "# 이후 값으로 채우기 (backward fill)\n",
    "# filled_df['x'] = filled_df['x'].bfill()\n",
    "# print(\"결측치를 이후 값으로 채운 후:\")\n",
    "\n",
    "plot_graph(filled_df, \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9a492",
   "metadata": {},
   "source": [
    "- 보간법(Interpolation) 사용하기\n",
    "    - 보간법은 결측치 주변의 값들을 기반으로 \"추정하여\" 값을 채우는 방법이다.\n",
    "    - interpolate() 함수를 사용하며, 특히 선형(linear) 보간법이 자주 쓰인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ce204",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df = missing_df.copy()\n",
    "\n",
    "# 선형 보간법으로 결측치 채우기\n",
    "filled_df['x'] = filled_df['x'].interpolate(method='linear')\n",
    "\n",
    "print(\"선형 보간법으로 결측치를 채운 후:\")\n",
    "plot_graph(filled_df, \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caead79c",
   "metadata": {},
   "source": [
    "### e. 기초실습 IV - 정규화, 표준화\n",
    "- 데이터 정규화와 표준화는 둘 다 데이터의 스케일을 조정하는 전처리 방법이지만, 목표와 방식에 차이가 있다.\n",
    "- 정규화 (Normalization)\n",
    "    - 정규화는 데이터의 범위를 0과 1 사이로 조정하는 것. 마치 점수를 100점 만점으로 통일하듯, 값의 최솟값을 0으로, 최댓값을 1로 바꾸고, 나머지 값들을 그 사이에 맞춰 변환한다.\n",
    "    - 목적: 데이터의 값을 특정 범위(주로 0과 1)로 맞춰서, 값이 너무 크거나 작아 학습에 불균형한 영향을 주는 것을 방지한다.\n",
    "    - 방법: '최소-최대 정규화(Min-Max Normalization)'가 가장 일반적이다.\n",
    "\n",
    "        <img src=\"images/Image_018.png\"><br>\n",
    "\n",
    "    - 특징: 데이터의 원래 분포는 그대로 유지된다.\n",
    "        - \"이상치(Outlier)\"에 매우 민감하다. 만약 데이터에 극단적으로 큰 값이 있다면, 다른 값들은 0에 가까운 매우 작은 값으로 몰리게 되어 변별력이 떨어질 수 있다. (반에 공부를 졸라 잘하는 놈이 있으면 모두가 바보가 되는 것과 같다. 더러운 세상...)\n",
    "    - 주요 사용처: 이미지 픽셀 값 조정(0~255를 0~1로), 딥러닝 모델의 입력 데이터 등, 데이터의 절대적인 스케일이 중요한 경우에 주로 사용한다.\n",
    "- 표준화 (Standardization)\n",
    "    - 표준화는 데이터의 분포를 평균이 0이고 표준편차가 1인 표준정규분포로 만드는 것이다. 이 과정은 마치 시험 점수를 평균과 표준편차를 이용해 'Z 점수'로 변환하는 것과 같다. (언제나 시험은 이모양이다. 빌어먹을.)\n",
    "    - 목적: 데이터의 평균을 0에 맞춰 중심을 이동시키고, 표준편차를 1로 만들어 데이터가 퍼진 정도를 일정하게 만든다.\n",
    "    - 방법: 'Z-점수 표준화(Z-score Standardization)'가 대표적이다.\n",
    "\n",
    "        <img src=\"images/image_019.png\"><br>\n",
    "        (μ는 평균, σ는 표준편차)\n",
    "\n",
    "    - 특징: 정규화와 달리 특정 범위로 제한되지 않는다.\n",
    "    - 이상치의 영향을 덜 받는다. 이상치가 있어도 모든 데이터를 평균 0을 중심으로 재배치하기 때문에 상대적으로 안정적이다.\n",
    "    - 주요 사용처: 선형 회귀, 로지스틱 회귀, PCA(주성분 분석) 등 통계적 가정을 기반으로 하는 머신러닝 모델이나, 데이터에 이상치가 많을 때 유용하다. (하지만 뭐 일률적인 것은 아니다. 해보고 결정해야 한다. 그러니까 노가다다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e429ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = read_data(\"data/hourly_stats.csv\")\n",
    "\n",
    "# # 일단 시간 형식으로 데이터를 변환합니다.\n",
    "# hourly_df['_time'] = pd.to_datetime(hourly_df['_time'], errors='coerce')\n",
    "# hourly_df.set_index('_time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df: pd.DataFrame, col_key):\n",
    "    \"\"\"\n",
    "    Min-Max 정규화를 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 정규화할 데이터프레임.\n",
    "        col_key (str): 정규화할 열의 이름.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 정규화된 데이터가 포함된 새로운 데이터프레임.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    min_val = df_copy[col_key].min()\n",
    "    max_val = df_copy[col_key].max()\n",
    "    df_copy[col_key] = (df_copy[col_key] - min_val) / (max_val - min_val)\n",
    "    return df_copy\n",
    "\n",
    "def standardize_data(df: pd.DataFrame, col_key):\n",
    "    \"\"\"\n",
    "    Z-스코어 표준화를 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 표준화할 데이터프레임.\n",
    "        col_key (str): 표준화할 열의 이름.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 표준화된 데이터가 포함된 새로운 데이터프레임.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    mean_val = df_copy[col_key].mean()\n",
    "    std_val = df_copy[col_key].std()\n",
    "    df_copy[col_key] = (df_copy[col_key] - mean_val) / std_val\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f746969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 시각화\n",
    "plot_graph(hourly_df, 'x')\n",
    "\n",
    "# 정규화된 데이터 시각화\n",
    "normalized_df = normalize_data(hourly_df, 'x')\n",
    "plot_graph(normalized_df, 'x')\n",
    "\n",
    "# 표준화된 데이터 시각화\n",
    "standardized_df = standardize_data(hourly_df, 'x')\n",
    "plot_graph(standardized_df, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce719459",
   "metadata": {},
   "source": [
    "- 한꺼번에 때려넣고 하니까 별 소득이 없다.\n",
    "    - 하루씩 끊어서 해볼까나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_day(df: pd.DataFrame, col_key):\n",
    "    \"\"\"\n",
    "    데이터를 하루 단위로 그룹화하여 Min-Max 정규화를 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 정규화할 데이터프레임. '_time' 컬럼이 datetime 객체여야 합니다.\n",
    "        col_key (str): 정규화할 열의 이름.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 하루 단위로 정규화된 데이터가 포함된 새로운 데이터프레임.\n",
    "    \"\"\"\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    # '_time' 컬럼을 datetime 형식으로 변환 (오류 방지를 위해 추가)\n",
    "    df_normalized['_time'] = pd.to_datetime(df_normalized['_time'], errors='coerce')\n",
    "\n",
    "    # '_time' 컬럼의 날짜(일자)를 기준으로 그룹화하고 각 그룹에 정규화 적용\n",
    "    def apply_normalize(group):\n",
    "        min_val = group[col_key].min()\n",
    "        max_val = group[col_key].max()\n",
    "        if max_val - min_val == 0:\n",
    "            group[col_key] = 0\n",
    "        else:\n",
    "            group[col_key] = (group[col_key] - min_val) / (max_val - min_val)\n",
    "        return group\n",
    "    \n",
    "    return df_normalized.groupby(df_normalized['_time'].dt.date).apply(apply_normalize)\n",
    "\n",
    "def standardize_by_day(df: pd.DataFrame, col_key):\n",
    "    \"\"\"\n",
    "    데이터를 하루 단위로 그룹화하여 Z-스코어 표준화를 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 표준화할 데이터프레임. 인덱스는 datetime 객체여야 합니다.\n",
    "        col_key (str): 표준화할 열의 이름.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 하루 단위로 표준화된 데이터가 포함된 새로운 데이터프레임.\n",
    "    \"\"\"\n",
    "    df_standardized = df.copy()\n",
    "    \n",
    "    # 날짜를 기준으로 그룹화하고 각 그룹에 표준화 적용\n",
    "    def apply_standardize(group):\n",
    "        mean_val = group[col_key].mean()\n",
    "        std_val = group[col_key].std()\n",
    "        if std_val == 0:\n",
    "            group[col_key] = 0\n",
    "        else:\n",
    "            group[col_key] = (group[col_key] - mean_val) / std_val\n",
    "        return group\n",
    "        \n",
    "    return df_standardized.groupby(df_standardized.index.date).apply(apply_standardize)\n",
    "\n",
    "# 표준화\n",
    "def normalize_by_day_zscroe(df:pd.DataFrame):\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    # '_time' 컬럼을 datetime 형식으로 변환\n",
    "    df_normalized['_time'] = pd.to_datetime(df_normalized['_time'])\n",
    "\n",
    "    # 날짜(일자) 컬럼 생성\n",
    "    df_normalized['date'] = df_normalized['_time'].dt.date\n",
    "\n",
    "    # 일자별로 그룹화하고 Z-점수 정규화 적용\n",
    "    df_normalized['x_normalized'] = df_normalized.groupby('date')['x'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "    return df_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4185627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 시각화\n",
    "plot_graph(hourly_df, 'x')\n",
    "\n",
    "# 정규화된 데이터 시각화\n",
    "standardized_df = normalize_by_day(df, 'x')\n",
    "print(standardized_df)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(standardized_df['_time'], standardized_df['x'], marker='o', linestyle='-', color='skyblue')\n",
    "\n",
    "# 그래프 제목과 라벨 설정\n",
    "plt.title('Stats by one minute', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('x', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 시각화\n",
    "plot_graph(hourly_df, 'x')\n",
    "\n",
    "# 표준화된 데이터 시각화\n",
    "normalized_df = normalize_by_day_zscroe(hourly_df)\n",
    "plot_graph(normalized_df, 'x_normalized')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95747e9",
   "metadata": {},
   "source": [
    "### f. 기초실습 III - 이상치 처리\n",
    "\n",
    "- 이상치를 발견하고 그걸 어떻게 처리할지 고민해보자.\n",
    "    - 없애야하나... 그래도 되나...\n",
    "- 이상치 식별 방법 🕵️‍♂️\n",
    "    - 통계적 방법:\n",
    "        - Z-점수(Z-score): 데이터가 평균에서 얼마나 떨어져 있는지 표준편차를 이용해서 가늠해본다. 보통 Z-score가 3보다 크면 이상치로 간주한다. (아니 그럴 수 있다. 물론 아닐 수도 있고...ㅠㅠ)\n",
    "    - 사분위수 범위(IQR, Interquartile Range): \n",
    "        - 데이터의 상위 25%와 하위 25% 사이의 범위를 이용. 0.25 ± 1.5 * IQR 범위를 주로 이용한다.\n",
    "- 시각화 방법:\n",
    "    - Box Plot: 데이터의 분포를 시각적으로 보여주며, IQR을 기준으로 이상치를 점으로 표시.\n",
    "    - 산점도(Scatter Plot): 두 변수 간의 관계를 시각화하여, 패턴에서 크게 벗어난 점을 찾는다.\n",
    "- 이상치 처리 방법 🛠️\n",
    "    - 이상치를 처리하는 방법은 이상치의 원인과 데이터의 특성에 따라 신중하게 결정해야. (하나마나한 소리...)\n",
    "    - 제거(Deletion): 이상치가 명백한 오류이거나 데이터가 충분히 많을 경우, 해당 이상치 데이터를 제거한다. 단, 데이터 손실이 발생할 수 있다. (버렸는데 당연히 손실이지... 나참.)\n",
    "    - 수정(Capping/Flooring): 이상치 값을 특정 임계값(예: Q_3+1.5×IQR)으로 대체하는 방법. 이는 데이터 손실 없이 이상치의 영향을 줄일 수 있다. 하지만 의도를 가진 수정이 될 수 있다. (외설과 예술의 경계는???)\n",
    "    - 변환(Transformation): 로그 변환이나 제곱근 변환 등을 통해 데이터의 분포를 정규분포에 가깝게 만들어 이상치의 영향을 완화한다.\n",
    "    - 대체(Imputation): 이상치를 제거하는 대신, 해당 값을 평균, 중앙값 또는 다른 통계적 값으로 대체한다.\n",
    "    - 유지(Retention): 이상치가 데이터의 자연스러운 변동성을 반영하는 경우(예: 특정 공정의 변화에는 필연적으로 수반되는 신호)에는 그대로 유지하는 것이 모델의 정확도를 높일 수도 있다.\n",
    "- 이상치를 잘못 처리하면 모델의 성능에 부정적인 영향을 미칠 수 있으므로, 항상 데이터의 맥락을 이해하고 적절한 방법을 선택하는 것이 중요하다.\n",
    "- OUtlier와 Anomaly를 혼동하지 말자!\n",
    "    - 이상치(outlier) 식별은 주로 단일 변수(Univariate) 또는 소수의 변수에서 데이터의 통계적 분포를 벗어난 극단적인 값을 찾아내는 데 초점을 맞춘다.\n",
    "        - 예를 들어, 열처리 온도를 측정하고 있는데 -290도 라는 값이 있다면 이는 물리적으로 명백한 오류이므로 이상치로 제거 되어야 한다.\n",
    "    - 이상 탐지는 더 넓은 개념으로, 여러 변수 간의 복합적인 관계나 데이터의 전체적인 패턴을 고려하여 정상적인 행동에서 벗어난 \"비정상적인\" 데이터 포인트를 찾아내는 데 초점을 맞춘다. 이는 단지 통계적 극단값뿐만 아니라, 예상치 못한 패턴이나 이벤트까지 포함한다.\n",
    "        - 예를 들어, 공작기계의 진동을 측정하고 있는데, 밤 11시에 진동신호가 측정된다면 이는 물리적으로 불가능한 극단값은 아니지만 통상적이지 않은 공정활동으로 탐지될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22fb05",
   "metadata": {},
   "source": [
    "- 우선 outlier detection (이상치 감지)를 먼저 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. IQR(사분위수 범위) 계산\n",
    "Q1 = normalized_df['x_normalized'].quantile(0.25)\n",
    "Q3 = normalized_df['x_normalized'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(f\"1사분위수 (Q1): {Q1}\")\n",
    "print(f\"3사분위수 (Q3): {Q3}\")\n",
    "print(f\"IQR: {IQR}\")\n",
    "\n",
    "# 3. 이상치 경계값 설정\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f\"하한 경계: {lower_bound}\")\n",
    "print(f\"상한 경계: {upper_bound}\")\n",
    "\n",
    "# 4. 이상치 탐지 및 제거\n",
    "outliers = normalized_df[(normalized_df['x_normalized'] < lower_bound) | (normalized_df['x_normalized'] > upper_bound)]\n",
    "df_cleaned = normalized_df[(normalized_df['x_normalized'] >= lower_bound) & (normalized_df['x_normalized'] <= upper_bound)]\n",
    "\n",
    "print(\"\\n--- 탐지된 이상치 ---\")\n",
    "print(outliers)\n",
    "\n",
    "print(\"\\n--- 이상치가 제거된 데이터 ---\")\n",
    "print(df_cleaned)\n",
    "\n",
    "def plot_graph_scale(df:pd.DataFrame, col_key):\n",
    "    # 1분 단위 평균 데이터를 그래프로 시각화합니다.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(df.index, df[col_key], marker='o', linestyle='-', color='skyblue')\n",
    "    # plt.scatter(df.index, df[col_key], linestyle='-', color='skyblue')\n",
    "\n",
    "    # 그래프 제목과 라벨 설정\n",
    "    plt.title('Stats by one minute', fontsize=16)\n",
    "    plt.xlabel('time', fontsize=12)\n",
    "    plt.ylabel('value', fontsize=12)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout() # 라벨이 잘리지 않도록 레이아웃 조정\n",
    "\n",
    "    # 그래프를 화면에 보여주기\n",
    "    plt.show()\n",
    "\n",
    "plot_graph_scale(normalized_df, \"x_normalized\")\n",
    "plot_graph_scale(df_cleaned, \"x_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d82b4",
   "metadata": {},
   "source": [
    "- 이번에는 anomaly detection (이상 감지.. 엥?)를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051b15f",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(contamination=0.13, random_state=42)\n",
    "model.fit(normalized_df[['x_normalized']])\n",
    "\n",
    "# 3. 이상치 예측 및 결과 추가\n",
    "# -1은 이상치, 1은 정상치를 의미합니다.\n",
    "normalized_df['anomaly'] = model.predict(normalized_df[['x_normalized']])\n",
    "print(normalized_df)\n",
    "\n",
    "# 4. 결과 시각화\n",
    "# 정상 데이터와 이상치를 시각적으로 구분하여 보여줍니다.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(normalized_df.index, normalized_df['x_normalized'], c=normalized_df['anomaly'], cmap='viridis')\n",
    "plt.title('Anomaly Detection using Isolation Forest')\n",
    "plt.xlabel('Data Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# 5. 이상치 데이터만 출력\n",
    "anomalies = normalized_df[normalized_df['anomaly'] == -1]\n",
    "print(\"\\n--- Identified Anomalies ---\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031ac1c",
   "metadata": {},
   "source": [
    "### g. 피처 엔지니어링 I - Fast Fourier Transformation\n",
    "\n",
    "- Time domain에서 Frequency domain으로 변환\n",
    "    - 데이터를 다른 각도에서 보는 것이다.\n",
    "    - 보이지 않던 특징을 볼 수 있을지도...\n",
    "\n",
    "- Fourier Transform (푸리에 변환)\n",
    "    - Fourier Transform은 시간 영역(time-domain)의 신호를 주파수 영역(frequency-domain)의 신호로 변환하는 수학적 기법이다.\n",
    "    - 복잡한 파동을 여러 개의 단순한 사인파와 코사인파의 합으로 분해하여 각 주파수 성분이 얼마나 포함되어 있는지 보여준다.\n",
    "        - 오케스트라의 연주음에서 각 악기(특정 주파수)의 소리를 분리해서 들을 수 있는 것과 유사.\n",
    "    - Fast Fourier Transform (FFT)\n",
    "        - FFT는 Fourier Transform을 수행하는 매우 빠른 알고리즘.\n",
    "        - 기존 Fourier Transform 연산법에서 계산수를 획기적으로 줄인 계산법\n",
    "            - 예를 들어, 1024개의 데이터를 변환할 때 DFT는 약 100만 번의 연산이 필요하지만, FFT는 1만 번의 연산만으로 충분.\n",
    "        - 수학적 차이점: FFT는 기존 Fourier Transform과 동일한 수학적 정의를 사용한다. 결과는 완벽하게 동일하고, 단지 연산의 순서를 효율적으로 재배열하여 계산량을 줄인 것뿐.\n",
    "            마치 덧셈을 할 때 1+2+3+4+5를 순서대로 계산하는 대신, (1+5)+(2+4)+3으로 묶어 계산하는 것과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8b636",
   "metadata": {},
   "source": [
    "- 데이터를 열어서 Fourier Transformation을 직접 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mpu6050_data(df:pd.DataFrame):\n",
    "    # 필요한 열이 존재하는지 확인합니다.\n",
    "    required_columns = ['acceleration_x', 'acceleration_y', 'acceleration_z']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"오류: CSV 파일에 필요한 열이 포함되어 있지 않습니다.\")\n",
    "        print(f\"예상되는 열: {required_columns}\")\n",
    "        return\n",
    "\n",
    "    # 원본 파형 플롯\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    ax.plot(df.index, df['acceleration_x'], label='Acceleration X', color='blue')\n",
    "    ax.plot(df.index, df['acceleration_y'], label='Acceleration Y', color='red')\n",
    "    ax.plot(df.index, df['acceleration_z'], label='Acceleration Z', color='green')\n",
    "\n",
    "    ax.set_title('Vibration wave form', fontsize=16)\n",
    "    ax.set_xlabel('Data index', fontsize=12)\n",
    "    ax.set_ylabel('Acceleration (g)', fontsize=12)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # # FFT 플롯\n",
    "    # plot_fft(df)\n",
    "\n",
    "    # 모든 플롯을 표시합니다.\n",
    "    plt.show()\n",
    "\n",
    "def plot_fft(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    FFT를 계산하고 가속도계 데이터의 주파수 스펙트럼을 플로팅합니다.\n",
    "    \"\"\"\n",
    "    # 샘플링 주파수 (예: 1초에 1000개의 데이터 포인트를 수집한다고 가정)\n",
    "    N = len(df)\n",
    "    T = 1.0 / 975.0  # 샘플링 간격\n",
    "    fs = 1.0 / T\n",
    "\n",
    "    # DC 오프셋(평균값)을 제거하여 0 Hz의 스파이크를 제거합니다.\n",
    "    accel_x_centered = df['acceleration_x'] - df['acceleration_x'].mean()\n",
    "    accel_y_centered = df['acceleration_y'] - df['acceleration_y'].mean()\n",
    "    accel_z_centered = df['acceleration_z'] - df['acceleration_z'].mean()\n",
    "\n",
    "    # 각 축에 대해 FFT를 수행합니다.\n",
    "    fft_x = np.fft.fft(accel_x_centered)\n",
    "    fft_y = np.fft.fft(accel_y_centered)\n",
    "    fft_z = np.fft.fft(accel_z_centered)\n",
    "\n",
    "    # 주파수 축을 계산합니다.\n",
    "    freq = np.fft.fftfreq(N, T)\n",
    "\n",
    "    # 양의 주파수만 취하여 양면 스펙트럼을 단면 스펙트럼으로 변환합니다.\n",
    "    n_positive = N // 2\n",
    "    freq_positive = freq[:n_positive]\n",
    "    fft_x_positive = 2.0/N * np.abs(fft_x[:n_positive])\n",
    "    fft_y_positive = 2.0/N * np.abs(fft_y[:n_positive])\n",
    "    fft_z_positive = 2.0/N * np.abs(fft_z[:n_positive])\n",
    "\n",
    "    # 주파수 스펙트럼을 플로팅합니다.\n",
    "    fig_fft, ax_fft = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    ax_fft.plot(freq_positive, fft_x_positive, label='FFT X', color='blue')\n",
    "    ax_fft.plot(freq_positive, fft_y_positive, label='FFT Y', color='red')\n",
    "    ax_fft.plot(freq_positive, fft_z_positive, label='FFT Z', color='green')\n",
    "\n",
    "    ax_fft.set_title('FFT results', fontsize=16)\n",
    "    ax_fft.set_xlabel('frequency (Hz)', fontsize=12)\n",
    "    ax_fft.set_ylabel('signal strength', fontsize=12)\n",
    "    ax_fft.legend(loc='upper right')\n",
    "    ax_fft.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 각 축의 대표 주파수 값을 찾아서 출력합니다.\n",
    "    dominant_freq_x_idx = np.argmax(fft_x_positive)\n",
    "    dominant_freq_y_idx = np.argmax(fft_y_positive)\n",
    "    dominant_freq_z_idx = np.argmax(fft_z_positive)\n",
    "\n",
    "    dominant_freq_x = freq_positive[dominant_freq_x_idx]\n",
    "    dominant_freq_y = freq_positive[dominant_freq_y_idx]\n",
    "    dominant_freq_z = freq_positive[dominant_freq_z_idx]\n",
    "\n",
    "    print(\"\\n--- 대표 주파수 ---\")\n",
    "    print(f\"X축: {dominant_freq_x:.2f} Hz\")\n",
    "    print(f\"Y축: {dominant_freq_y:.2f} Hz\")\n",
    "    print(f\"Z축: {dominant_freq_z:.2f} Hz\")\n",
    "    print(\"------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 플로팅할 CSV 파일의 이름\n",
    "csv_file = \"mpu6050_data.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "plot_mpu6050_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6464da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edb0ca",
   "metadata": {},
   "source": [
    "- 실제 데이터로 한번 해볼까나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373cfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 플로팅할 CSV 파일의 이름\n",
    "csv_file = \"data/Accelermeter_SEN002_2023-05-16T07.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "plot_graph(df, \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 주파수 (예: 1초에 1000개의 데이터 포인트를 수집한다고 가정)\n",
    "N = len(df)\n",
    "T = 1.0 / 50.0  # 샘플링 간격\n",
    "fs = 1.0 / T\n",
    "\n",
    "# DC 오프셋(평균값)을 제거하여 0 Hz의 스파이크를 제거합니다.\n",
    "x_centered = df['x'] - df['x'].mean()\n",
    "\n",
    "# FFT를 수행합니다.\n",
    "fft_x = np.fft.fft(x_centered)\n",
    "\n",
    "# 주파수 축을 계산합니다.\n",
    "freq = np.fft.fftfreq(N, T)\n",
    "\n",
    "# 양의 주파수만 취하여 양면 스펙트럼을 단면 스펙트럼으로 변환합니다.\n",
    "n_positive = N // 2\n",
    "freq_positive = freq[:n_positive]\n",
    "fft_x_positive = 2.0/N * np.abs(fft_x[:n_positive])\n",
    "\n",
    "# 주파수 스펙트럼을 플로팅합니다.\n",
    "fig_fft, ax_fft = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax_fft.plot(freq_positive, fft_x_positive, label='FFT X', color='blue')\n",
    "\n",
    "ax_fft.set_title('FFT results', fontsize=16)\n",
    "ax_fft.set_xlabel('frequency (Hz)', fontsize=12)\n",
    "ax_fft.set_ylabel('signal strength', fontsize=12)\n",
    "ax_fft.legend(loc='upper right')\n",
    "ax_fft.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 각 축의 대표 주파수 값을 찾아서 출력합니다.\n",
    "dominant_freq_x_idx = np.argmax(fft_x_positive)\n",
    "\n",
    "dominant_freq_x = freq_positive[dominant_freq_x_idx]\n",
    "\n",
    "print(\"\\n--- 대표 주파수 ---\")\n",
    "print(f\"my data: {dominant_freq_x:.2f} Hz\")\n",
    "print(\"------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05701783",
   "metadata": {},
   "source": [
    "### h. 피처 엔지니어링 II - Spectrogram\n",
    "\n",
    "- 암튼, 나는 모르겠고... 컴퓨터야 네가 한번 보렴... 넌 참으로 뛰어나잖아, 그치?\n",
    "- \"그럼, spectrogram을 이용해보는 건 어때요?\" 엥????\n",
    "- Spectrogram: 신호의 주파수 성분이 시간에 따라 어떻게 변하는지 시각적으로 나타낸 그래프\n",
    "    - 특히 오디오, 지진파, 의료 신호(심전도 등)와 같은 비정상(non-stationary) 신호, 즉 주파수가 시간에 따라 변하는 신호를 분석하는 데 매우 유용하다.\n",
    "    - 기본적으로 신호를 작은 시간 조각(segment)으로 나눈 뒤, 각 조각에 대해 Fourier Transform을 수행하여 주파수 성분을 분석한다.\n",
    "        - 가로축 (X축): 시간 ⏱️\n",
    "        - 세로축 (Y축): 주파수 🎶\n",
    "        - 색상 또는 밝기: 특정 시간-주파수 조합에서의 신호의 강도(amplitude) 또는 에너지 💡\n",
    "    - Spectrogram을 생성하는 핵심은 \"Short-Time Fourier Transform (STFT)\".\n",
    "        - 일반적인 Fourier Transform은 전체 신호에 대해 한 번에 주파수 분석을 수행하여 \"어떤 주파수 성분들이 존재하는가\"를 알려주지만, \"언제\" 그 주파수들이 나타나는지는 알려주지 않는다.\n",
    "        - STFT는 \"짧은 시간 윈도우(window)\"를 적용하고, 이 윈도우를 신호 전체에 걸쳐 이동시키면서 각 윈도우 내의 신호에 대해 푸리에 변환을 반복적으로 수행함으로써 시간에 따른 주파수 분포의 변화를 포착할 수 있다.\n",
    "    - 활용 분야\n",
    "        - 음성 및 오디오 분석: 사람의 목소리나 음악에서 특정 음정, 음색, 억양 등을 시각적으로 분석하는 데 사용. 음성 인식 시스템의 핵심 기술 중 하나.\n",
    "        - 레이더 및 소나: 물체와의 거리나 속도를 측정하는 신호에서 주파수 변화를 분석.\n",
    "        - 지진학: 지진파의 주파수 특성 변화를 분석하여 지진의 종류나 원인을 파악.\n",
    "        - 의학: 심전도(ECG)나 뇌전도(EEG) 신호의 주파수 변화를 분석하여 질병을 진단.\n",
    "    - Spectrogram의 장점\n",
    "        - Spectrogram은 복잡한 시계열 데이터를 직관적인 2D 이미지 형태로 변환하므로 CNN(Convolutional Neural Network)과 같은 딥러닝 모델에 입력 데이터로 사용하기 매우 적합하다.\n",
    "            - CNN은 이미지의 공간적 특징을 잘 학습하므로, spectrogram의 시간-주파수 패턴을 효율적으로 인식할 수 있다. 🤖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793785c",
   "metadata": {},
   "source": [
    "- 아, 됐고... 암튼 한번 해보자고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48384aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_from_df(df, sampling_rate=50):\n",
    "    \"\"\"\n",
    "    Takes a pandas DataFrame with an 'x' column, computes a spectrogram,\n",
    "    and returns the spectrogram image as a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing time-series data.\n",
    "        sampling_rate (int): The number of data points per second. Default is 50.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array representing the spectrogram image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the required 'x' column exists\n",
    "    if 'x' not in df.columns:\n",
    "        raise ValueError(\"The input DataFrame must contain an 'x' column.\")\n",
    "\n",
    "    # Extract the 'x' column data\n",
    "    data = df['x'].values\n",
    "    \n",
    "    # Create a figure and axes without displaying them\n",
    "    fig, ax = plt.subplots(figsize=(10, 5)) \n",
    "\n",
    "    # Compute the spectrogram\n",
    "    ax.specgram(data, NFFT=256, Fs=sampling_rate, noverlap=128, cmap='viridis')\n",
    "    \n",
    "    # --- Remove axes, ticks, labels, and title ---\n",
    "    ax.set_axis_off()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    \n",
    "    # Save the plot to an in-memory buffer\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Close the figure to free up memory\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Rewind the buffer to the beginning\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Open the image from the buffer and convert it to a NumPy array\n",
    "    img = Image.open(buf)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Close the buffer\n",
    "    buf.close()\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def save_spectrogram_array_with_matplotlib(spectrogram_array, output_path):\n",
    "    \"\"\"\n",
    "    Saves a NumPy array representing an image to a file using Matplotlib.\n",
    "\n",
    "    Args:\n",
    "        spectrogram_array (np.ndarray): The NumPy array of the spectrogram image.\n",
    "        output_path (str): The path to save the output file (e.g., 'spectrogram.png').\n",
    "    \"\"\"\n",
    "    # 새로운 figure를 생성합니다.\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # imshow 함수를 사용하여 배열을 이미지로 표시합니다.\n",
    "    ax.imshow(spectrogram_array)\n",
    "\n",
    "    # 축과 경계선을 제거하여 순수한 이미지만 저장되도록 합니다.\n",
    "    ax.set_axis_off()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "    # 이미지를 파일로 저장합니다.\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # 메모리 해제를 위해 figure를 닫습니다.\n",
    "    plt.close(fig)\n",
    "    print(f\"Spectrogram image saved to {output_path}\")\n",
    "\n",
    "def plot_graph_simple(df:pd.DataFrame, col_key):\n",
    "    # 1분 단위 평균 데이터를 그래프로 시각화합니다.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(df.index, df[col_key], marker='o', linestyle='-', color='skyblue')\n",
    "    # plt.scatter(df.index, df[col_key], linestyle='-', color='skyblue')\n",
    "\n",
    "    # 그래프 제목과 라벨 설정\n",
    "    plt.title('Simple Data View', fontsize=16)\n",
    "    plt.xlabel('time', fontsize=12)\n",
    "    plt.ylabel('value', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout() # 라벨이 잘리지 않도록 레이아웃 조정\n",
    "\n",
    "    # 그래프를 화면에 보여주기\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ec7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/Accelermeter_SEN002_2023-05-16T07.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "spectrogram_array = create_spectrogram_from_df(df)\n",
    "save_spectrogram_array_with_matplotlib(spectrogram_array, 'Accelermeter_SEN002_2023-05-16T07.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf08a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "plot_graph_simple(df, \"x\")\n",
    "\n",
    "# 저장된 이미지를 열어서 봅니다.\n",
    "img = Image.open('Accelermeter_SEN002_2023-05-16T07.png')\n",
    "\n",
    "# Display the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"data/by_hour\"\n",
    "output_folder = \"data/spectrograms\"\n",
    "\n",
    "# 파일 목록을 가져옵니다.\n",
    "csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    spectrogram_array = create_spectrogram_from_df(df)\n",
    "    # Get the output file path\n",
    "    output_filename = os.path.splitext(os.path.basename(csv_file))[0] + '.png'\n",
    "    output_path = os.path.join(output_folder, output_filename)\n",
    "    save_spectrogram_array_with_matplotlib(spectrogram_array, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3abe86",
   "metadata": {},
   "source": [
    "- 그럼 이걸 가지고 뭘 어떻게 한다는 건가?\n",
    "    - \"궁금하면 500원!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d5cbc",
   "metadata": {},
   "source": [
    "### i. 피처 엔지니어링 III - Clustering\n",
    "\n",
    "- 뭘해도 보이지가 않는다...\n",
    "    - 그럼 할 수 없다. 그냥.... 다 때려넣고 흔들어... 언제까지? 뭔가 나올때까지...!!\n",
    "\n",
    "        <img src=\"images/image_machine_learning.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d548604",
   "metadata": {},
   "source": [
    "- 일단 spectrogram을 얻은 것은 큰 진전(??!!)\n",
    "\n",
    "    <img src=\"images/image_021.png\"><br>\n",
    "\n",
    "- 그런데 데이터를 보니 전반적으로 이런식!!\n",
    "\n",
    "    <img src=\"images/image_022.png\" width=\"600\"><br>\n",
    "\n",
    "- 혹시 이거 뭐 두가지 정도의 데이터로 구분되는 거 아닌가?\n",
    "    - 하루치 데이터를 한시간 단위로 spectrogram을 만들고 그걸 비교해보자.\n",
    "\n",
    "        <img src=\"images/image_023.png\" width=\"600\"><br>\n",
    "\n",
    "    - 그래서 그 두 그룹간의 거리가 매일 조금씩 달라지는 것은 아닐까?\n",
    "\n",
    "        <img src=\"images/image_024.png\" width=\"600\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e3aca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
